{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZztHOf7tmLU"
      },
      "source": [
        "# KAIST AI605 Assignment 2: Text Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbGnNWI1lRy_"
      },
      "source": [
        "## Environment\n",
        "You will only use Python 3.7 and PyTorch 1.9, which is already available on Colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYxEp1XMpxem",
        "outputId": "fb2ced16-e3e9-40b0-8e05-61ece10078b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python 3.7.12\n",
            "torch 1.9.0+cu111\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from textwrap import wrap, indent\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from platform import python_version\n",
        "from IPython.display import clear_output\n",
        "\n",
        "print(f\"python {python_version()}\")\n",
        "print(f\"torch {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eut41a9MCyAI"
      },
      "source": [
        "You will use two datasets, namely SQuAD, for retrieval.\n",
        "Note that this is a MRC dataset but we will view them as retrieval task by trying to find the correct document corresponding to the question among all the documents in the **validation** data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r4Dzd3zoDSKR"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import datasets\n",
        "except ImportError:\n",
        "    !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72hC4Q8fDf7W",
        "outputId": "7d27b2b1-874c-4c13-a94d-302e6a6558f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "  Super Bowl 50 was an American football game to determine the champion\n",
            "  of the National Football League (NFL) for the 2015 season. The\n",
            "  American Football Conference (AFC) champion Denver Broncos defeated\n",
            "  the National Football Conference (NFC) champion Carolina Panthers\n",
            "  24â€“10 to earn their third Super Bowl title. The game was played on\n",
            "  February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at\n",
            "  Santa Clara, California. As this was the 50th Super Bowl, the league\n",
            "  emphasized the \"golden anniversary\" with various gold-themed\n",
            "  initiatives, as well as temporarily suspending the tradition of naming\n",
            "  each Super Bowl game with Roman numerals (under which the game would\n",
            "  have been known as \"Super Bowl L\"), so that the logo could prominently\n",
            "  feature the Arabic numerals 50.\n",
            "Question:\n",
            "  Which NFL team represented the AFC at Super Bowl 50?\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "squad = load_dataset(\"squad\")\n",
        "valid0 = squad[\"validation\"][0]\n",
        "clear_output()\n",
        "print(f\"Context:\")\n",
        "print(indent(\"\\n\".join(wrap(valid0[\"context\"])), \"  \"))\n",
        "print(f\"Question:\")\n",
        "print(indent(\"\\n\".join(wrap(valid0[\"question\"])), \"  \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lQVpklafINLO"
      },
      "outputs": [],
      "source": [
        "def preprocess(squad):\n",
        "    context_dict = {}\n",
        "    documents = []\n",
        "    query_pairs = []\n",
        "    count = 0\n",
        "\n",
        "    for context, question in zip(squad[\"context\"], squad[\"question\"]):\n",
        "        context = context.strip()\n",
        "        question = question.strip()\n",
        "        if context in context_dict:\n",
        "            context_id = context_dict[context]\n",
        "        else:\n",
        "            context_id = count\n",
        "            context_dict[context] = count\n",
        "            count += 1\n",
        "            documents.append(context)\n",
        "        query_pairs.append((context_id, question))\n",
        "    return documents, query_pairs\n",
        "\n",
        "squad_documents, squad_queries = preprocess(squad[\"validation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB7xyzIgnnkA"
      },
      "source": [
        "## 1. Measuring Similarity\n",
        "We discussed in Lecture 08 that there are several ways to measure similarity between two vectors,\n",
        "such as L2 (Euclidean) distance, L1 (Manhattan) distance, inner product, and cosine distance.\n",
        "Here, except for inner product, all other measures are [*metric*](https://en.wikipedia.org/wiki/Metric_(mathematics))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FBaabvtl_xz"
      },
      "source": [
        "> A function $d: X \\times X \\to \\mathbb{R}$ is said to be a metric on $X$ if:\n",
        "> - (*Non-negativity*) $d(\\mathbf{x}, \\mathbf{y}) \\geq 0$ for all $\\mathbf{x}, \\mathbf{y} \\in X$\n",
        "> \n",
        "> And satisfies following three axioms:\n",
        "> 1. (*Identity of Indiscernibility*) $d(\\mathbf{x}, \\mathbf{y}) = 0 \\iff \\mathbf{x} = \\mathbf{y}$\n",
        "> 2. (*Symmetry*) $d(\\mathbf{x}, \\mathbf{y}) = d(\\mathbf{y}, \\mathbf{x})$ for all $\\mathbf{x}, \\mathbf{y} \\in X$\n",
        "> 3. (*Triangle Inequality*) $d(\\mathbf{x}, \\mathbf{y}) \\leq d(\\mathbf{x}, \\mathbf{z}) + d(\\mathbf{z}, \\mathbf{y})$ for all $\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in X$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_LOyd1iOKsG"
      },
      "source": [
        "> **Problem 1.1** *(2 points)* Using the definition of metric above, prove that L1 distance is a metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0tKeFUluXXI"
      },
      "source": [
        "> **Solution 1.1**\n",
        "> Let $\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathbb{R}^n$ and $\\mathbf{x} = (x_1, \\dots, x_n), \\mathbf{y} = (y_1, \\dots, y_n), \\mathbf{z} = (z_1, \\dots, z_n)$.\n",
        "> \n",
        "> L1 distance is defined as\n",
        "> $$\n",
        "    d(\\mathbf{x}, \\mathbf{y})\n",
        "        := |\\mathbf{x} - \\mathbf{y}|\n",
        "         = \\sum_{i=1}^n |x_i - y_i|. $$\n",
        "> To prove that L1 distance is a metric on $\\mathbb{R}^n$,\n",
        "> we should show L1 distance is non-negative and satisfies above three properties.\n",
        "> \n",
        "> (*Non-negativity*)\n",
        "> By the non-negative property of the absolute value,\n",
        "> $$\n",
        "    |x_i - y_i| \\geq 0 \\, \\text{ for } \\, i = 1, \\dots, n. $$\n",
        "> Then,\n",
        "> $$\n",
        "    \\begin{align*}\n",
        "        d(\\mathbf{x}, \\mathbf{y})\n",
        "        &= \\sum_{i=1}^n |x_i - y_i| \\\\\n",
        "        &\\geq \\sum_{i=1}^n 0 \\\\\n",
        "        &= 0.\n",
        "    \\end{align*} $$\n",
        "> So, $d(\\mathbf{x}, \\mathbf{y}) \\geq 0$ for all $\\mathbf{x}, \\mathbf{y} \\in \\mathbf{R}^n$ and L1 distance is non-negative.\n",
        "> \n",
        "> (*Identity of Indiscernibility*)\n",
        "> By the non-negative property of the absolute value,\n",
        "> $$\n",
        "    d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^n |x_i - y_i| = 0\n",
        "    \\iff |x_i - y_i| = 0 \\,\\text{ for }\\, i = 1, \\dots, n. $$\n",
        "> And, by the identity of indiscernibles property of the absolute value,\n",
        "> $$\n",
        "    \\begin{align*}\n",
        "        |x_i - y_i| = 0 \\,\\text{ for }\\, i = 1, \\dots, n\n",
        "        &\\iff x_i = y_i \\,\\text{ for }\\, i = 1, \\dots, n \\\\\n",
        "        &\\iff \\mathbf{x} = \\mathbf{y}.\n",
        "    \\end{align*} $$\n",
        "> So, $d(\\mathbf{x}, \\mathbf{y}) = 0 \\iff \\mathbf{x} = \\mathbf{y}$ and L1 distance satisfies the identity of indiscernibles axiom.\n",
        "> \n",
        "> (*Symmetry*)\n",
        "> By the definition of the absolute value,\n",
        "> $$\n",
        "    \\begin{align*}\n",
        "        d(\\mathbf{x}, \\mathbf{y})\n",
        "        &= \\sum_{i=1}^n |x_i - y_i| \\\\\n",
        "        &= \\sum_{i=1}^n |-(x_i - y_i)| \\\\\n",
        "        &= \\sum_{i=1}^n |y_i - x_i| \\\\\n",
        "        &= d(\\mathbf{y}, \\mathbf{x}).\n",
        "    \\end{align*} $$\n",
        "> So, $d(\\mathbf{x}, \\mathbf{y}) = d(\\mathbf{y}, \\mathbf{x})$ for all $\\mathbf{x}, \\mathbf{y} \\in \\mathbf{R}^n$ and L1 distance satisfies the symmetry axiom.\n",
        ">\n",
        "> (*Triangle Inequality*)\n",
        "> By the triangle inequality property of the absolute value,\n",
        "> $$\n",
        "    |(x_i - z_i) + (z_i - y_i)| \\leq |x_i - z_i| + |z_i - y_i|\\, \\text{ for }\\, i = 1, \\dots, n. $$\n",
        "> Then,\n",
        "> $$\n",
        "    \\begin{align*}\n",
        "        d(\\mathbf{x}, \\mathbf{y})\n",
        "        &= \\sum_{i=1}^n |x_i - y_i| \\\\\n",
        "        &= \\sum_{i=1}^n |(x_i - z_i) + (z_i - y_i)| \\\\\n",
        "        &\\leq \\sum_{i=1}^n (|x_i - z_i| + |z_i - y_i|) \\\\\n",
        "        &\\leq \\sum_{i=1}^n |x_i - z_i| + \\sum_{i=1}^n |z_i - y_i| \\\\\n",
        "        &= d(\\mathbf{x}, \\mathbf{z}) + d(\\mathbf{z}, \\mathbf{y}).\n",
        "    \\end{align*} $$\n",
        "> So, $d(\\mathbf{x}, \\mathbf{y}) \\leq d(\\mathbf{x}, \\mathbf{z}) + d(\\mathbf{z}, \\mathbf{y})$ for all $\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathbf{R}^n$ and L1 distance satisfies the triangle inequality axiom.\n",
        ">\n",
        "> Finally, we show that L1 distance is non-negative and satisfies three axioms of a metric.\n",
        "> Thus, L1 distance is a metric on $\\mathbf{R}^n$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yND_V75w94Pd"
      },
      "source": [
        "> **Problem 1.2** *(2 points)* Prove that negative inner product is NOT a metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd1DiIrNuY3h"
      },
      "source": [
        "> **Solution 1.2**\n",
        "> Let $\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathbb{R}^n$ and $\\mathbf{x} = (x_1, \\dots, x_n), \\mathbf{y} = (y_1, \\dots, y_n), \\mathbf{z} = (z_1, \\dots, z_n)$.\n",
        "> \n",
        "> Negative inner product is defined as\n",
        "> $$\n",
        "    d(\\mathbf{x}, \\mathbf{y})\n",
        "        := - \\langle\\mathbf{x}, \\mathbf{y}\\rangle\n",
        "         = - \\sum_{i=1}^n x_i y_i. $$\n",
        "> \n",
        "> To prove that negative inner product is not a metric on $\\mathbb{R}^n$,\n",
        "> we should show negative inner product is not non-negative or not satisfies one of above three properties.\n",
        ">\n",
        "> Let assume $\\mathbf{x} = (1, 1), \\mathbf{y} = (0, 0)$. Then,\n",
        "> $$\n",
        "    \\begin{align*}\n",
        "        d(\\mathbf{x}, \\mathbf{y})\n",
        "             &= - \\sum_{i=1}^n x_i y_i \\\\\n",
        "             &= - (1 \\cdot 0 + 1 \\cdot 0) \\\\\n",
        "             &= 0.\n",
        "    \\end{align*} $$\n",
        "> It means there exists $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ that NOT $d(\\mathbf{x}, \\mathbf{y}) = 0 \\iff \\mathbf{x} = \\mathbf{y}$.\n",
        "> Thus, negative inner product is not a metric because it does not satisfy the identity of indiscernibles axiom."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFcaA8GI-B7X"
      },
      "source": [
        "> **Problem 1.3** *(2 points)* Prove that cosine distance (1 - cosine similarity) is NOT a metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6PQeuP_uZfO"
      },
      "source": [
        "> **Solution 1.3**\n",
        "> Let $\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathbb{R}^n$ and $\\mathbf{x} = (x_1, \\dots, x_n), \\mathbf{y} = (y_1, \\dots, y_n), \\mathbf{z} = (z_1, \\dots, z_n)$.\n",
        "> \n",
        "> Cosine distance is defined as\n",
        "> $$\n",
        "    d(\\mathbf{x}, \\mathbf{y})\n",
        "        := 1 - \\cos(\\theta)\n",
        "         = 1 - \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\Vert\\mathbf{x}\\Vert \\Vert\\mathbf{y}\\Vert}\n",
        "         = 1 - \\frac{\\sum_{i=1}^n x_i y_i}{\\sqrt{\\sum_{i=1}^n x_i^2} \\sqrt{\\sum_{i=1}^n y_i^2}}. $$\n",
        "> \n",
        "> To prove that cosine distance is not a metric on $\\mathbb{R}^n$,\n",
        "> we should show cosine distance is not non-negative or not satisfies one of above three properties.\n",
        ">\n",
        "> Let assume $\\mathbf{x} = (1, 0), \\mathbf{y} = (0, 1), \\mathbf{z} = (1, 1)$.\n",
        "> Then,\n",
        "> $$\n",
        "    \\begin{align*}\n",
        "        d(\\mathbf{x}, \\mathbf{y})\n",
        "        &= 1 - \\frac{\\sum_{i=1}^n x_i y_i}{\\sqrt{\\sum_{i=1}^n x_i^2} \\sqrt{\\sum_{i=1}^n y_i^2}} \\\\\n",
        "        &= 1 - \\frac{1 \\cdot 0 + 0 \\cdot 1}{\\sqrt{1^2 + 0^2} \\sqrt{0^2 + 1^2}} \\\\\n",
        "        &= 1.\n",
        "    \\end{align*} $$\n",
        "> And,\n",
        "> $$\n",
        "    \\begin{align*}\n",
        "        d(\\mathbf{x}, \\mathbf{z}) + d(\\mathbf{z}, \\mathbf{y})\n",
        "        &= \\left(1 - \\frac{\\sum_{i=1}^n x_i z_i}{\\sqrt{\\sum_{i=1}^n x_i^2} \\sqrt{\\sum_{i=1}^n z_i^2}} \\right) + \\left(1 - \\frac{\\sum_{i=1}^n z_i y_i}{\\sqrt{\\sum_{i=1}^n z_i^2} \\sqrt{\\sum_{i=1}^n y_i^2}} \\right) \\\\\n",
        "        &= \\left(1 - \\frac{1 \\cdot 1 + 0 \\cdot 1}{\\sqrt{1^2 + 0^2} \\sqrt{1^2 + 1^2}}\\right) + \\left(1 - \\frac{0 \\cdot 1 + 1 \\cdot 1}{\\sqrt{1^2 + 1^2} \\sqrt{0^2 + 1^2}}\\right)\\\\\n",
        "        &= 2 - \\sqrt{2} \\\\\n",
        "        &< 1.\n",
        "    \\end{align*} $$\n",
        "> It means there exists $\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathbf{R}^n$ that $d(\\mathbf{x}, \\mathbf{y}) > d(\\mathbf{x}, \\mathbf{z}) + d(\\mathbf{z}, \\mathbf{y})$.\n",
        "> Thus, cosine distance is not a metric because it does not safisfy the triangular inequality axiom."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfSr80bq-Gr6"
      },
      "source": [
        "> **Problem 1.4 (bonus)** *(3 points)*\n",
        "Given a model that can perform nearest neighbor search in L2 space,\n",
        "can you modify your query and your key vectors to perform maximum inner product search?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQaf4RDUucd7"
      },
      "source": [
        "> **Solution 1.4**\n",
        "> Let query be $\\mathbf{q} = (q_1, \\dots, q_n) \\in \\mathbb{R}^n$,\n",
        "> key be $\\mathbf{k} = (k_1, \\dots, k_n) \\in \\mathbb{R}^n$,\n",
        "> and the set of candidate keys be $K = \\{\\mathbf{k} | \\mathbf{k} \\in \\mathbb{R}^n\\} \\subseteq \\mathbb{R}^n$.\n",
        ">\n",
        "> L2 NNS is defined as\n",
        "> $$\n",
        "    \\begin{align*}\n",
        "        \\text{NNS}(\\textbf{q}, K)\n",
        "            &:= \\argmin_{\\textbf{k} \\in K} \\Vert \\textbf{q} - \\textbf{k} \\Vert_2^2 \\\\\n",
        "            &= \\argmin_{\\textbf{k} \\in K} \\sqrt{\\sum_{i=1}^n (q_i - k_i)^2} \\\\\n",
        "            &= \\argmin_{\\textbf{k} \\in K} \\sum_{i=1}^n (q_i - k_i)^2 \\\\\n",
        "            &= \\argmin_{\\textbf{k} \\in K} \\sum_{i=1}^n (- q_i k_i + \\frac{1}{2}k_i^2) \\\\\n",
        "            &= \\argmax_{\\textbf{k} \\in K} (\\sum_{i=1}^n q_i k_i - \\frac{1}{2} \\Vert \\textbf{k} \\Vert_2^2).\n",
        "    \\end{align*} $$\n",
        "> And MIPS is defined as\n",
        "> $$\n",
        "    \\begin{align*}\n",
        "        \\text{MIPS}(\\textbf{q}, K)\n",
        "            &:= \\argmax_{\\textbf{k} \\in K} \\langle \\textbf{q}, \\textbf{k} \\rangle \\\\\n",
        "            &= \\argmax_{\\textbf{k} \\in K} \\sum_{i=1}^n q_i k_i.\n",
        "    \\end{align*} $$\n",
        "> Notice that L2 NNS becomes MIPS when $\\Vert \\textbf{k} \\Vert_2^2$ is constant for all $\\textbf{k} \\in K$.\n",
        "> And it means the norm of key vectors should be same.\n",
        ">\n",
        "> Now, let the largest norm of key vectors be $d = \\max_{\\textbf{k} \\in K} \\Vert\\textbf{k}\\Vert_2$.\n",
        "> And let the new key be $\\tilde{\\textbf{k}} = (\\frac{\\textbf{k}}{d}, \\frac{1}{2} - \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^2, \\frac{1}{2} - \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^4, \\dots, \\frac{1}{2} - \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^{2^m})$,\n",
        "> and the new query be $\\tilde{\\textbf{q}} = (\\textbf{q}, 0, 0, \\dots, 0)$ which has same dimension with $\\tilde{\\textbf{k}}$.\n",
        "> Then L2 NNS of the new query and new keys becomes:\n",
        "> $$\n",
        "    \\begin{align*}\n",
        "        \\text{NNS}(\\tilde{\\textbf{q}}, \\tilde{K})\n",
        "            &= \\argmax_{\\textbf{k} \\in K} (\\sum_{i=1}^{n + m} \\tilde{q}_i \\tilde{k}_i - \\frac{1}{2} \\Vert\\tilde{\\textbf{k}}\\Vert_2^2) \\\\\n",
        "            &= \\argmax_{\\textbf{k} \\in K} (\\sum_{i=1}^n q_i k_i - \\frac{d}{2} \\Vert\\tilde{\\textbf{k}}\\Vert_2^2).\n",
        "    \\end{align*} $$\n",
        "> And,\n",
        "> $$\n",
        "    \\begin{align*}\n",
        "        \\Vert\\tilde{\\textbf{k}}\\Vert_2^2\n",
        "            &= \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^2 + (\\frac{1}{2} - \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^2)^2 + (\\frac{1}{2} - \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^4)^2 + \\cdots + (\\frac{1}{2} - \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^{2^m})^2 \\\\\n",
        "            &= \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^2 + \\frac{m}{4} - (\\Vert\\frac{\\textbf{k}}{d}\\Vert_2^2 + \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^4 + \\cdots + \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^{2^m}) + (\\Vert\\frac{\\textbf{k}}{d}\\Vert_2^4 + \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^8 + \\cdots + \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^{2^{m+1}}) \\\\\n",
        "            &= \\frac{m}{4} + \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^{2^{m+1}}.\n",
        "    \\end{align*} $$\n",
        "> When $m$ goes to infinity, the norm of the new key becomes:\n",
        "> $$\n",
        "    \\Vert\\tilde{\\textbf{k}}\\Vert_2^2 = \\frac{m}{4} + \\Vert\\frac{\\textbf{k}}{d}\\Vert_2^{2^{m+1}} \\approx \\frac{m}{4}, $$\n",
        "> since $0 \\leq \\Vert\\frac{\\textbf{k}}{d}\\Vert_2 \\leq 1$.\n",
        "> So, the norm of the new key is constant for all $\\textbf{k} \\in K$. \n",
        "> Again, L2 NNS becomes:\n",
        "> $$\n",
        "    \\begin{align*}\n",
        "        \\text{NNS}(\\tilde{\\textbf{q}}, \\tilde{K})\n",
        "            &= \\argmax_{\\textbf{k} \\in K} (\\sum_{i=1}^n q_i k_i - \\frac{d}{2} \\Vert\\tilde{\\textbf{k}}\\Vert_2^2) \\\\\n",
        "            &\\approx \\argmax_{\\textbf{k} \\in K} (\\sum_{i=1}^n q_i k_i - \\frac{dm}{8}) \\\\\n",
        "            &= \\argmax_{\\textbf{k} \\in K} \\sum_{i=1}^n q_i k_i \\\\\n",
        "            &= \\text{MIPS}(\\textbf{q}, K).\n",
        "    \\end{align*} $$\n",
        "> So, we can perform MIPS with a model performs L2 NNS.\n",
        ">\n",
        "> Reference: [Asymmetric LSH(ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)](https://arxiv.org/pdf/1405.5869.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr94hZkt-dc9"
      },
      "source": [
        "We first create an abstract class for performing similarity search as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9iXGXr7i-8MD"
      },
      "outputs": [],
      "source": [
        "class SimilaritySearch:\n",
        "    def __init__(self):\n",
        "        self.is_trained = False\n",
        "\n",
        "    def _check_trained(self):\n",
        "        if not self.is_trained:\n",
        "            raise RuntimeError(\"Model should be trained before adding documents\")\n",
        "\n",
        "    # Make vocab from documents\n",
        "    def train(self, documents: list):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    # Add documents (a list of text)\n",
        "    def add(self, documents: list):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    # Returns the indices of top-k documents among the added documents\n",
        "    # that are most similar to the input query \n",
        "    def search(self, query: str, k: int) -> list:\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryvHK75tE5JO"
      },
      "source": [
        "You will use the same space-based tokenizer that you used in Assignment 1, with lowercasing to make it case insensitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rZ9hYdXtCKwM"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence):\n",
        "    tokens = [word for word in re.split(r\"\\W+\", sentence.lower()) if word]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F1E-W04Bxow"
      },
      "source": [
        "## 2. Sparse Search\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "47rWqUlll_yB"
      },
      "outputs": [],
      "source": [
        "def recall(model: SimilaritySearch, queries: list, k: int):\n",
        "    hit, total = 0, 0\n",
        "    for idx, query in queries:\n",
        "        topk_idx = model.search(query, k)\n",
        "        hit += (1 if idx in topk_idx else 0)\n",
        "        total += 1\n",
        "    return hit / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cjdOzOaBt0k"
      },
      "source": [
        "> **Problem 2.1** *(2 points)*\n",
        "We will first start with Bag of Words that we discussed in Lecture 08.\n",
        "Using the definition in the class, implement `BagOfWords` class that subclasses `SimilaritySearch` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UUEuPZx-l_yC"
      },
      "outputs": [],
      "source": [
        "class BagOfWords(SimilaritySearch):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vocab = defaultdict(int)\n",
        "        self.vocab2idx = defaultdict(lambda: -1)\n",
        "        self.dtm = None\n",
        "\n",
        "    def _normalize(self, vector: torch.tensor) -> torch.tensor:\n",
        "        norm_vector = vector / torch.norm(vector)\n",
        "        return norm_vector\n",
        "\n",
        "    def _embed(self, document: str) -> torch.tensor:\n",
        "        counts = Counter(tokenize(document))\n",
        "        idxs = [self.vocab2idx[token] for token in counts]\n",
        "        cols = torch.tensor([[idx for idx in idxs if idx >= 0]], requires_grad=False)\n",
        "        values = torch.tensor([count for idx, count in zip(idxs, counts.values())\n",
        "                               if idx >= 0], dtype=float, requires_grad=False)\n",
        "        size = [len(self.vocab)]\n",
        "        embed_vector = torch.sparse_coo_tensor(cols, self._normalize(values),\n",
        "                                               size, requires_grad=False)\n",
        "        return embed_vector\n",
        "\n",
        "    def train(self, documents: list):\n",
        "        self.dtm = None\n",
        "        for document in documents:\n",
        "            for token in set(tokenize(document)):\n",
        "                self.vocab[token] += 1\n",
        "        self.vocab2idx.update({token: idx for idx, token in enumerate(self.vocab)})\n",
        "        self.is_trained = True\n",
        "\n",
        "    def add(self, documents: list):\n",
        "        self._check_trained()\n",
        "        sub_dtm = torch.stack([self._embed(document) for document in documents])\n",
        "        self.dtm = sub_dtm if self.dtm is None else torch.cat((self.dtm, sub_dtm))\n",
        "        self.dtm = self.dtm.to_dense()  # For speed up\n",
        "\n",
        "    def search(self, query: str, k: int) -> list:\n",
        "        query_vector = self._embed(query).coalesce()\n",
        "        query_idxs, query_values = query_vector.indices(), query_vector.values()\n",
        "        dtm = self.dtm.index_select(dim=1, index=query_idxs[0])\n",
        "        scores = torch.matmul(dtm, query_values)\n",
        "        scores = (scores.to_dense() if scores.is_sparse else scores)\n",
        "        topk_idx = scores.topk(k).indices.tolist()\n",
        "        return topk_idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIMu5VLNl_yC",
        "outputId": "d4508665-dd5a-4e5b-dd97-f88a180cd539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 24, 1, 28, 36, 398, 1000, 1373, 12, 850]\n"
          ]
        }
      ],
      "source": [
        "bow = BagOfWords()\n",
        "bow.train(squad_documents)\n",
        "bow.add(squad_documents)\n",
        "topk_idx = bow.search(squad_documents[0], 10)\n",
        "print(topk_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp0RJ5c5l_yE",
        "outputId": "0c0b1942-dd7b-442f-a1ac-fe9f50f66b3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall@10 of BagOfWords: 0.5562\n"
          ]
        }
      ],
      "source": [
        "print(f\"Recall@10 of BagOfWords: {recall(bow, squad_queries, 10):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVbZm1HhCeQh"
      },
      "source": [
        "> **Problem 2.2** *(2 points)* Using the definition in Lecture 08, implement `TFIDF` class that subclasses `BagOfWords` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1CiS5RfZvSGQ"
      },
      "outputs": [],
      "source": [
        "class TFIDF(BagOfWords):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.idf = None\n",
        "\n",
        "    def _normalize(self, vector: torch.tensor) -> torch.tensor:\n",
        "        norm_vector = vector / torch.sum(vector)\n",
        "        return norm_vector\n",
        "\n",
        "    def _scores(self, tf: torch.tensor, idf: torch.tensor) -> torch.tensor:\n",
        "        return torch.sum((tf * idf), dim=1)\n",
        "\n",
        "    def train(self, documents: list):\n",
        "        super().train(documents)\n",
        "        df = torch.tensor(list(self.vocab.values()), dtype=float)\n",
        "        self.idf = torch.log(len(documents) / df).unsqueeze(dim=0)\n",
        "\n",
        "    def search(self, query: str, k: int) -> list:\n",
        "        words = set(tokenize(query))\n",
        "        query_idxs = torch.tensor([idx for idx, token in enumerate(self.vocab)\n",
        "                                   if token in words])\n",
        "        tf = self.dtm.index_select(dim=1, index=query_idxs)\n",
        "        tf = (tf.to_dense() if tf.is_sparse else tf)\n",
        "        idf = self.idf.index_select(dim=1, index=query_idxs)\n",
        "        scores = self._scores(tf, idf)\n",
        "        topk_idx = scores.topk(k).indices.tolist()\n",
        "        return topk_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4zj6Rfsl_yF",
        "outputId": "f4235332-0467-4529-eced-b9051c19bc7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 5, 7, 24, 4, 23, 25, 8, 22, 26]\n"
          ]
        }
      ],
      "source": [
        "tfidf = TFIDF()\n",
        "tfidf.train(squad_documents)\n",
        "tfidf.add(squad_documents)\n",
        "topk_idx = tfidf.search(squad_documents[0], 10)\n",
        "print(topk_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw_uPvvIEPbk"
      },
      "source": [
        "> **Problem 2.3** *(2 points)* Use `TFIDF` to measure the recall rate of the correct document when 10 documents (contexts) are retrieved (this is called **Recall@10**) in SQuAD **validation** set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHrVzfpul_yG",
        "outputId": "7115bb83-0362-4da7-e850-4780d93d3cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall@10 of TFIDF: 0.8605\n"
          ]
        }
      ],
      "source": [
        "print(f\"Recall@10 of TFIDF: {recall(tfidf, squad_queries, 10):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDNZ0yJKEf9C"
      },
      "source": [
        "> **Problem 2.4 (bonus)** *(2 points)* Implement `BM25` that sublcasses `BagOfWords` and repeat Problem 2.3 for BM25."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rWbMdGFZvPyn"
      },
      "outputs": [],
      "source": [
        "class BM25(TFIDF):\n",
        "    def __init__(self, k1: float = 1.2, b: float = 0.75):\n",
        "        super().__init__()\n",
        "        self.dl = None\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "    \n",
        "    def _scores(self, tf: torch.tensor, idf: torch.tensor) -> torch.tensor:\n",
        "        return torch.sum(idf * ((tf * (self.k1 + 1) / (tf + self.k1 * self.dlw))), dim=1)\n",
        "\n",
        "    def train(self, documents: list):\n",
        "        super().train(documents)\n",
        "        dl = torch.tensor([len(tokenize(document)) for document in documents], dtype=float)\n",
        "        self.dlw = (1 - self.b + self.b * dl / torch.mean(dl)).unsqueeze(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhTp8Bl-l_yI",
        "outputId": "3d214f23-d3a9-4ed0-e49c-79cff3da11e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 5, 23, 4, 7, 20, 25, 472, 24, 2]\n"
          ]
        }
      ],
      "source": [
        "bm25 = BM25()\n",
        "bm25.train(squad_documents)\n",
        "bm25.add(squad_documents)\n",
        "topk_idx = bm25.search(squad_documents[0], 10)\n",
        "print(topk_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeH70eiRl_yJ",
        "outputId": "a943338e-5634-49bd-d9be-0c839a30be5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall@10 of BM25: 0.7365\n"
          ]
        }
      ],
      "source": [
        "print(f\"Recall@10 of BM25: {recall(bm25, squad_queries, 10):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E78UAz3HEyfZ"
      },
      "source": [
        "## 3. Dense Search\n",
        "\n",
        "To obtain the embedding of each document and query, you will use GloVe embedding (Pennington et al., 2014).\n",
        "Go to [GloVe project page](https://nlp.stanford.edu/projects/glove/) and download `glove.6B.zip`.\n",
        "You are also welcome to use other more convenient ways to access the GloVe embeddings.\n",
        "You will compute the document's embedding by simply averaging the embeddings of all words in the document (same for the query), and then normalizing it.\n",
        "This way, inner product effectively becomes cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "I0ZjySwZwBDR"
      },
      "outputs": [],
      "source": [
        "from urllib import request\n",
        "import zipfile\n",
        "\n",
        "if not os.path.exists(\"glove/glove.6B.100d.txt\"):\n",
        "    os.makedirs(\"glove\", exist_ok=True)\n",
        "    print(\"Downloading glove.6B.zip ... \")\n",
        "    request.urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", \"glove/glove.6B.zip\")\n",
        "    with zipfile.ZipFile(\"glove/glove.6B.zip\", \"r\") as zipf:\n",
        "        zipf.extractall(\"glove\")\n",
        "    clear_output()\n",
        "\n",
        "dim = 300\n",
        "\n",
        "with open(f\"glove/glove.6B.{dim}d.txt\", \"r\") as glove:\n",
        "    glove_map = map(lambda line: line.split(), glove.readlines())\n",
        "    to_tensor = lambda float_list: torch.tensor(list(map(float, float_list)))\n",
        "    word2vec = {word: to_tensor(vec) for word, *vec in glove_map}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "M3mk6aNNvUMH"
      },
      "outputs": [],
      "source": [
        "class GloVeAverage(SimilaritySearch):\n",
        "    def __init__(self, word2vec: dict):\n",
        "        super().__init__()\n",
        "        self.word2vec = word2vec\n",
        "        self.dtm = None\n",
        "\n",
        "    def _embed(self, document: str) -> torch.tensor:\n",
        "        word_vecs = torch.vstack([self.word2vec[word] for word in tokenize(document)\n",
        "                                  if word in word2vec])\n",
        "        vector = torch.mean(word_vecs, dim=0)\n",
        "        embed_vector = vector / torch.norm(vector)\n",
        "        return embed_vector\n",
        "\n",
        "    def train(self, documents: list):\n",
        "        self.dtm = None\n",
        "        self.is_trained = True  # No need to make vocab\n",
        "\n",
        "    def add(self, documents: list):\n",
        "        self._check_trained()\n",
        "        sub_dtm = torch.stack([self._embed(document) for document in documents])\n",
        "        self.dtm = sub_dtm if self.dtm is None else torch.cat((self.dtm, sub_dtm))\n",
        "\n",
        "    def search(self, query: str, k: int) -> list:\n",
        "        query_vector = self._embed(query)\n",
        "        scores = torch.matmul(self.dtm, query_vector)\n",
        "        topk_idx = scores.topk(k).indices.tolist()\n",
        "        return topk_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7vQwkh2l_yM",
        "outputId": "6fb83743-1adc-40c6-fbb5-3f331b9fb1b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 8, 9, 6, 18, 24, 20, 7, 19]\n"
          ]
        }
      ],
      "source": [
        "gva = GloVeAverage(word2vec)\n",
        "gva.train(squad_documents)\n",
        "gva.add(squad_documents)\n",
        "topk_idx = gva.search(squad_documents[0], 10)\n",
        "print(topk_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE9PJg8AGbYV"
      },
      "source": [
        "> **Problem 3.2** *(2 points)* Use `GloVeAverage` to measure the recall at 10 for SQuAD validation dataset. How does it compare to TFIDF?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQTtU0M6vU5K",
        "outputId": "b2af4156-b86a-4912-d2fa-20e8ae7e1e9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall@10 of GloVeAverage: 0.6833\n"
          ]
        }
      ],
      "source": [
        "print(f\"Recall@10 of GloVeAverage: {recall(gva, squad_queries, 10):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XveLZaWKl_yP"
      },
      "source": [
        "> It gives much lower score than TFIDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MpUySZMGqUc"
      },
      "source": [
        "> **Problem 3.3** *(2 points)*\n",
        "Implement `GloVeAverageFaiss` that subclasses `SimilaritySearch` and uses Faiss `IndexFlatIP` instead of PyTorch native tensor operation for search.\n",
        "Refer to the [Faiss wiki](https://github.com/facebookresearch/faiss/wiki/Getting-started) for instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "V5Tg0Ge-l_yQ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import faiss\n",
        "except ImportError:\n",
        "    !pip install faiss-cpu\n",
        "    import faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2EgHsBZfvVZX"
      },
      "outputs": [],
      "source": [
        "class GloVeAverageFaiss(GloVeAverage):\n",
        "    def train(self, documents: list):\n",
        "        self.dtm = faiss.IndexFlatIP(dim)\n",
        "        self.is_trained = True\n",
        "\n",
        "    def add(self, documents: list):\n",
        "        self._check_trained()\n",
        "        sub_dtm = torch.stack([self._embed(document) for document in documents])\n",
        "        self.dtm.add(sub_dtm.numpy())\n",
        "\n",
        "    def search(self, query: str, k: int) -> list:\n",
        "        query_vector = self._embed(query).unsqueeze(dim=0)\n",
        "        scores = self.dtm.search(query_vector.numpy(), k)\n",
        "        topk_idx = scores[1][0].tolist()\n",
        "        return topk_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y61usbxJl_yR",
        "outputId": "c5e09595-d2c5-4e23-9f32-eddd9f7ed39b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 8, 9, 6, 18, 24, 20, 7, 19]\n"
          ]
        }
      ],
      "source": [
        "gvaf = GloVeAverageFaiss(word2vec)\n",
        "gvaf.train(squad_documents)\n",
        "gvaf.add(squad_documents)\n",
        "topk_idx = gvaf.search(squad_documents[0], 10)\n",
        "print(topk_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu5Hcsy_l_yS",
        "outputId": "bc95ee0d-febf-45d4-bef0-2309195289ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall@10 of GloVeAverageFaiss: 0.6833\n"
          ]
        }
      ],
      "source": [
        "print(f\"Recall@10 of GloVeAverageFaiss: {recall(gvaf, squad_queries, 10):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CJT5YnlHU29"
      },
      "source": [
        "> **Problem 3.4** *(1 points)*\n",
        "Compare the speed between `GloVeAverage` and `GloVeAverageFaiss` on SQuAD.\n",
        "To make the measurement accurate, perform search many times (at least more than 1000) and take the average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5-0Y8FdvV_R",
        "outputId": "c7bd39d0-135b-4740-b478-51599712b425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GloVeAverage:      0.237 ms / query\n",
            "GloVeAverageFaiss: 1.089 ms / query\n"
          ]
        }
      ],
      "source": [
        "import timeit\n",
        "from itertools import cycle\n",
        "\n",
        "def benchmark(model, queries: list, k: int = 10, num_iter: int = 100000):\n",
        "    iter_q = cycle(query for _, query in queries)\n",
        "    total = timeit.timeit(lambda: model.search(next(iter_q), k), number=num_iter)\n",
        "    return total / num_iter * 1000\n",
        "\n",
        "print(f\"GloVeAverage:      {benchmark(gva, squad_queries):.3f} ms / query\")\n",
        "print(f\"GloVeAverageFaiss: {benchmark(gvaf, squad_queries):.3f} ms / query\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "text-retrieval.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "6d0e1d92a6aeba30a80b51107bd9fa84acf5c90c2355e7662e835b3e5879c85b"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('egyun': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
